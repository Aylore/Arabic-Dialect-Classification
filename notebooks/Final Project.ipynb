{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-6lrKz6orxT"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:08.379056Z",
     "start_time": "2023-05-14T16:53:08.373729Z"
    },
    "id": "eXUPo3g4orxV"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import re\n",
    "from sklearn import metrics\n",
    "from lightgbm import LGBMClassifier\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fG8MkuvjorxX"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:11.400625Z",
     "start_time": "2023-05-14T16:53:11.397238Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:27.915561Z",
     "start_time": "2023-05-14T16:53:27.402739Z"
    },
    "id": "BYeqhp66orxY"
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"../data/dialects_database.db\")\n",
    "df_label = pd.read_sql_query(\"SELECT * FROM id_text\", conn)\n",
    "df_target = pd.read_sql_query(\"SELECT * FROM id_dialect\", conn)\n",
    "df = pd.merge(df_label, df_target, on=\"id\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save as csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:15.981149Z",
     "start_time": "2023-05-14T16:53:15.467484Z"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/dialects.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T13:02:46.402106Z",
     "start_time": "2023-05-14T13:02:46.396971Z"
    }
   },
   "source": [
    "**Read csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:17.568734Z",
     "start_time": "2023-05-14T16:53:17.563877Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:25.732037Z",
     "start_time": "2023-05-14T16:53:25.724782Z"
    }
   },
   "outputs": [],
   "source": [
    "# ls = [row[1:] for row in csv.reader(open('../data/dialects.csv'))]\n",
    "# df = pd.DataFrame(ls)\n",
    "# df.columns = df.iloc[0]\n",
    "# df = df.drop(0, axis=0)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-13T22:28:04.387683Z",
     "start_time": "2023-05-13T22:28:04.382688Z"
    }
   },
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:31.867210Z",
     "start_time": "2023-05-14T16:53:31.861710Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:33.723852Z",
     "start_time": "2023-05-14T16:53:33.529386Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df[\"text\"]\n",
    "y = df[\"dialect\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:36.558344Z",
     "start_time": "2023-05-14T16:53:36.506764Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "MAX_WORDS = 10_000\n",
    "INPUT_LENGTH = MAX_SEQUENCE_LEN = max(len(sentence) for sentence in X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:53:49.630169Z",
     "start_time": "2023-05-14T16:53:43.974106Z"
    }
   },
   "outputs": [],
   "source": [
    "def wrangle_dl(df):\n",
    "    #spilt \n",
    "    X = df[\"text\"]\n",
    "    y = df[\"dialect\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "    \n",
    "    #preprocess\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "    \n",
    "    tok = Tokenizer(num_words=MAX_WORDS)\n",
    "    tok.fit_on_texts(X_train)\n",
    "\n",
    "    sequences = tok.texts_to_sequences(X_train)\n",
    "    X_train_padded = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LEN)\n",
    "    y_train_ = to_categorical(y_train)\n",
    "\n",
    "    test_sequences = tok.texts_to_sequences(X_test)\n",
    "    X_test_padded = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LEN)\n",
    "    y_test_ = to_categorical(y_test)\n",
    "\n",
    "    return X_train_padded, X_test_padded, y_train_, y_test_\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = wrangle_dl(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:20:59.673636Z",
     "start_time": "2023-05-14T16:20:59.520027Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 698, 64)           640000    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 673,349\n",
      "Trainable params: 673,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(MAX_WORDS, 64, input_length=INPUT_LENGTH),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "#     tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:21:02.192075Z",
     "start_time": "2023-05-14T16:21:02.171097Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy', tfa.metrics.F1Score(average='macro', num_classes=NUM_CLASSES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:39:06.979248Z",
     "start_time": "2023-05-14T16:21:02.732878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4155/4155 [==============================] - 1084s 261ms/step - loss: 0.5482 - accuracy: 0.8039 - f1_score: 0.7618\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=1, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:39:57.055981Z",
     "start_time": "2023-05-14T16:39:10.320081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462/462 [==============================] - 47s 100ms/step - loss: 0.4510 - accuracy: 0.8417 - f1_score: 0.8115\n",
      "F1 score for testing : 0.8114644885063171\n"
     ]
    }
   ],
   "source": [
    "print(f'F1 score for testing : {model.evaluate(X_test, y_test)[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:40:07.520440Z",
     "start_time": "2023-05-14T16:40:04.985454Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/LSTM/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/LSTM/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('../models/LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:40:57.437417Z",
     "start_time": "2023-05-14T16:40:08.803811Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462/462 [==============================] - 47s 101ms/step - loss: 0.4510 - accuracy: 0.8417 - f1_score: 0.8115\n",
      "F1 score for testing : 0.8114644885063171\n"
     ]
    }
   ],
   "source": [
    "# model = tf.saved_model.load(\"../models/SimpleRNN/\")\n",
    "model = tf.keras.models.load_model('../models/LSTM/')\n",
    "print(f'F1 score for testing : {model.evaluate(X_test, y_test)[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T07:44:00.576773Z",
     "start_time": "2023-05-14T07:44:00.573321Z"
    }
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:09:22.850005Z",
     "start_time": "2023-05-14T16:09:22.849997Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def remove_user(text: str) -> str:\n",
    "    return re.sub(r\"@\\w+\", \" \", text)\n",
    "\n",
    "def replace_spaces(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(text))\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    text = remove_user(text)\n",
    "    text = replace_spaces(text)\n",
    "    return text\n",
    "\n",
    "def wrangle_ml(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"text\"] = df[\"text\"].apply(preprocess)\n",
    "    X = df[\"text\"]\n",
    "    y = df[\"dialect\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:09:22.851499Z",
     "start_time": "2023-05-14T16:09:22.851490Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:09:22.854072Z",
     "start_time": "2023-05-14T16:09:22.854050Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:09:22.855813Z",
     "start_time": "2023-05-14T16:09:22.855804Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_list = stopwords.words('arabic')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:09:22.857628Z",
     "start_time": "2023-05-14T16:09:22.857617Z"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "def fit_ml(X_train, X_test, y_train, y_test):\n",
    "    final_model = LinearSVC(random_state=42)\n",
    "    pipe = Pipeline([(\"Vectorizer\", TfidfVectorizer(ngram_range=(1, 2), stop_words='arabic')), (\"classifier\", LinearSVC(random_state=42))])\n",
    "    pipe.fit(X_train, y_train)\n",
    "#     joblib.dump(pipe, \"models/ml_model.pkl\")\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:09:22.859381Z",
     "start_time": "2023-05-14T16:09:22.859372Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = wrangle_ml(df)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_prep = le.fit_transform(y_train)\n",
    "y_test_prep = le.transform(y_train)\n",
    "\n",
    "model = fit_ml(X_train, X_test, y_train_prep, y_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:09:22.860107Z",
     "start_time": "2023-05-14T16:09:22.860099Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def eval_ml(pipe, X_test, y_test):\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_pred = le.inverse_transform(y_pred)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"ML Macro F1 score for testing: {f1_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-14T16:09:22.861078Z",
     "start_time": "2023-05-14T16:09:22.861070Z"
    }
   },
   "outputs": [],
   "source": [
    "eval_ml(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
